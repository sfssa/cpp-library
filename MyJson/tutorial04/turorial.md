# 从零开始的JSON库教程（四）：Unicode

## Unicode

首先了解下ASCII，它是一种字符编码，把 128 个字符映射至整数 0 ~ 127。例如，`1` → 49，`A` → 65，`B` → 66 等等。这种 7-bit 字符编码系统非常简单，在计算机中以一个字节存储一个字符。然而，它仅适合美国英语，甚至一些英语中常用的标点符号、重音符号都不能表示，无法表示各国语言，特别是中日韩语等表意文字。

在Unicode出现之前，各个国家有不同的编码，当多语言混合编程时非常不便，于是有了Unicode编码。

Unicode 是一种字符编码标准，旨在涵盖世界上几乎所有书写系统中的所有字符，以及一些控制字符和特殊符号。它的目标是提供一个唯一的标识符，使计算机能够准确地识别和处理文本，而不受语言、国家或文化的限制。以下是关于 Unicode 编码的详细信息：

1. **字符表示**：Unicode 使用唯一的数字值（码点）来表示每个字符。这些码点通常用十六进制表示，前缀为 U+，例如 U+0041 表示字符 "A"。
2. **字符集**：Unicode 定义了一个庞大的字符集，包括各种语言的字母、数字、标点符号、控制字符和特殊符号。这个字符集被组织成一些不同的面（planes），每个面包含若干字符。其中，基本多文本平面（Basic Multilingual Plane，BMP）包含了常用的字符，如拉丁字母、汉字等。
3. **UTF 编码**：Unicode 字符通常被编码为字节序列以在计算机中存储和传输。最常用的编码方案之一是 UTF-8，它使用变长字节数来表示字符。UTF-16 和 UTF-32 也是常见的编码方案。UTF-8 编码以一种节省空间的方式表示字符，英文字符通常使用一个字节，汉字和其他字符可能需要多个字节。
4. **字符范围**：Unicode 字符集包括了从 U+0000 到 U+10FFFF 的范围，其中 U+0000 到 U+FFFF 属于 BMP，而 U+10000 到 U+10FFFF 属于辅助平面（Supplementary Planes）。这种安排允许 Unicode 表示几乎所有已知书写系统的字符。
5. **字符的应用**：Unicode 广泛用于各种计算机应用，包括文本处理、编程、数据库存储、互联网通信和操作系统。Unicode 的使用使得不同语言和字符集之间的文本交换变得更容易，同时也支持跨文化和国际化的应用程序开发。
6. **标准化**：Unicode 编码标准由 Unicode Consortium 组织维护和更新。该组织定期发布 Unicode 标准的新版本，以包括新的字符和特性。
7. **字符检索**：Unicode 字符可以通过其十六进制码点进行检索。编程语言和库通常提供函数和方法来处理 Unicode 字符串，使您能够访问、搜索和操作文本中的字符。

这些字符被收录为统一字符集（Universal Coded Character Set, UCS），每个字符映射至一个整数码点（code point），码点的范围是 0 至 0x10FFFF，码点又通常记作 U+XXXX，当中 XXXX 为 16 进位数字。例如 `劲` → U+52B2、`峰` → U+5CF0。很明显，UCS 中的字符无法像 ASCII 般以一个字节存储。

因此，Unicode 还制定了各种储存码点的方式，这些方式称为 Unicode 转换格式（Uniform Transformation Format, UTF）。现时流行的 UTF 为 UTF-8、UTF-16 和 UTF-32。每种 UTF 会把一个码点储存为一至多个编码单元（code unit）。例如 UTF-8 的编码单元是 8 位的字节、UTF-16 为 16 位、UTF-32 为 32 位。除 UTF-32 外，UTF-8 和 UTF-16 都是可变长度编码。

UTF-8 成为现时互联网上最流行的格式，有几个原因：

1. 它采用字节为编码单元，不会有字节序（endianness）的问题。
2. 每个 ASCII 字符只需一个字节去储存。
3. 如果程序原来是以字节方式储存字符，理论上不需要特别改动就能处理 UTF-8 的数据。

## 需求

由于 UTF-8 的普及性，大部分的 JSON 也通常会以 UTF-8 存储。我们的 JSON 库也会只支持 UTF-8。对于非转义（unescaped）的字符，只要它们不少于 32（0 ~ 31 是不合法的编码单元），我们可以直接复制至结果，这一点我们稍后再说明。我们假设输入是以合法 UTF-8 编码。

而对于 JSON字符串中的 `\uXXXX` 是以 16 进制表示码点 U+0000 至 U+FFFF，我们需要：

1. 解析 4 位十六进制整数为码点；
2. 由于字符串是以 UTF-8 存储，我们要把这个码点编码成 UTF-8。

同学可能会发现，4 位的 16 进制数字只能表示 0 至 0xFFFF，但之前我们说 UCS 的码点是从 0 至 0x10FFFF，那怎么能表示多出来的码点？

其实，U+0000 至 U+FFFF 这组 Unicode 字符称为基本多文种平面（basic multilingual plane, BMP），还有另外 16 个平面。那么 BMP 以外的字符，JSON 会使用代理对（surrogate pair）表示 `\uXXXX\uYYYY`。在 BMP 中，保留了 2048 个代理码点。如果第一个码点是 U+D800 至 U+DBFF，我们便知道它的代码对的高代理项（high surrogate），之后应该伴随一个 U+DC00 至 U+DFFF 的低代理项（low surrogate）。然后，我们用下列公式把代理对 (H, L) 变换成真实的码点：

```
codepoint = 0x10000 + (H − 0xD800) × 0x400 + (L − 0xDC00)
```

举个例子，高音谱号字符 `𝄞` → U+1D11E 不是 BMP 之内的字符。在 JSON 中可写成转义序列 `\uD834\uDD1E`，我们解析第一个 `\uD834` 得到码点 U+D834，我们发现它是 U+D800 至 U+DBFF 内的码点，所以它是高代理项。然后我们解析下一个转义序列 `\uDD1E` 得到码点 U+DD1E，它在 U+DC00 至 U+DFFF 之内，是合法的低代理项。我们计算其码点：

```
H = 0xD834, L = 0xDD1E
codepoint = 0x10000 + (H − 0xD800) × 0x400 + (L − 0xDC00)
          = 0x10000 + (0xD834 - 0xD800) × 0x400 + (0xDD1E − 0xDC00)
          = 0x10000 + 0x34 × 0x400 + 0x11E
          = 0x10000 + 0xD000 + 0x11E
          = 0x1D11E
```

这样就得出这转义序列的码点，然后我们再把它编码成 UTF-8。如果只有高代理项而欠缺低代理项，或是低代理项不在合法码点范围，我们都返回 `LEPT_PARSE_INVALID_UNICODE_SURROGATE` 错误。如果 `\u` 后不是 4 位十六进位数字，则返回 `LEPT_PARSE_INVALID_UNICODE_HEX` 错误。

## utf-8编码

由于我们的 JSON 库也只支持 UTF-8，我们需要把码点编码成 UTF-8。这里简单介绍一下 UTF-8 的编码方式。UTF-8 的编码单元为 8 位（1 字节），每个码点编码成 1 至 4 个字节。它的编码方式很简单，按照码点的范围，把码点的二进位分拆成 1 至最多 4 个字节：

| 码点范围           | 码点位数 | 字节1    | 字节2    | 字节3    | 字节4    |
| ------------------ | -------- | -------- | -------- | -------- | -------- |
| U+0000 ~ U+007F    | 7        | 0xxxxxxx |          |          |          |
| U+0080 ~ U+07FF    | 11       | 110xxxxx | 10xxxxxx |          |          |
| U+0800 ~ U+FFFF    | 16       | 1110xxxx | 10xxxxxx | 10xxxxxx |          |
| U+10000 ~ U+10FFFF | 21       | 11110xxx | 10xxxxxx | 10xxxxxx | 10xxxxxx |

这个编码方法的好处之一是，码点范围 U+0000 ~ U+007F 编码为一个字节，与 ASCII 编码兼容。这范围的 Unicode 码点也是和 ASCII 字符相同的。因此，一个 ASCII 文本也是一个 UTF-8 文本。

我们举一个例子解析多字节的情况，欧元符号 `€` → U+20AC：

1. U+20AC 在 U+0800 ~ U+FFFF 的范围内，应编码成 3 个字节。
2. U+20AC 的二进位为 10000010101100
3. 3 个字节的情况我们要 16 位的码点，所以在前面补两个 0，成为 0010000010101100
4. 按上表把二进位分成 3 组：0010, 000010, 101100
5. 加上每个字节的前缀：11100010, 10000010, 10101100
6. 用十六进位表示即：0xE2, 0x82, 0xAC

对于这例子的范围，对应的 C 代码是这样的：

```
if (u >= 0x0800 && u <= 0xFFFF) {
    OutputByte(0xE0 | ((u >> 12) & 0xFF)); /* 0xE0 = 11100000 */
    OutputByte(0x80 | ((u >>  6) & 0x3F)); /* 0x80 = 10000000 */
    OutputByte(0x80 | ( u        & 0x3F)); /* 0x3F = 00111111 */
}
```

UTF-8 的解码稍复杂一点，但我们的 JSON 库不会校验 JSON 文本是否符合 UTF-8，所以这里也不展开了。

## 实现\uXXXX解析

我们只需要在其它转义符的处理中加入对 `\uXXXX` 的处理：

```
static int lept_parse_string(lept_context* c, lept_value* v) {
    unsigned u;
    /* ... */
    for (;;) {
        char ch = *p++;
        switch (ch) {
            /* ... */
            case '\\':
                switch (*p++) {
                    /* ... */
                    case 'u':
                        if (!(p = lept_parse_hex4(p, &u)))
                            STRING_ERROR(LEPT_PARSE_INVALID_UNICODE_HEX);
                        /* \TODO surrogate handling */
                        lept_encode_utf8(c, u);
                        break;
                    /* ... */
                }
            /* ... */
        }
    }
}
```

上面代码的过程很简单，遇到 `\u` 转义时，调用 `lept_parse_hex4()` 解析 4 位十六进数字，存储为码点 `u`。这个函数在成功时返回解析后的文本指针，失败返回 `NULL`。如果失败，就返回 `LEPT_PARSE_INVALID_UNICODE_HEX` 错误。最后，把码点编码成 UTF-8，写进缓冲区。

为 `lept_parse_string()` 做了个简单的重构，把返回错误码的处理抽取为宏：

```
#define STRING_ERROR(ret) do { c->top = head; return ret; } while(0)
```

现在来看下lept_parse_hex4函数：

```
static const char* lept_parse_hex4(const char* p,unsigned* u)
{
    int i;
    *u=0;
    for(i=0;i<4;++i)
    {
        char ch=*p++;
        *u<<4;
        if(ch>='0'&&ch<='9')
            *u |=ch='0';
        else if(ch>='A'&&ch<='F')
            *u |=ch-('A'-10);
        else if(ch>='a'&&ch<='f')
            *u |=ch-('a'-10);
        else
            return nullptr;
    }
    return p;
}
```

```
static const char* lept_parse_hex4(const char* p, unsigned* u)
```

- `p` 是一个指向 JSON 字符串的指针，该字符串包含一个 4 位的十六进制数字（即 Unicode 字符的码点）。
- `u` 是一个无符号整数指针，用于存储解析后的码点。

```
for (i = 0; i < 4; ++i)
{
    char ch = *p++;
    *u << 4;
```

接下来，它进入一个循环，循环 4 次，每次处理一个字符。在每次迭代中，它执行以下操作：

1. 从 JSON 字符串中读取一个字符，存储在变量 `ch` 中。
2. 左移码点 `u` 的 4 位，相当于将当前码点的每一位向左移动 4 位，为新的十六进制数位腾出位置。

```
if (ch >= '0' && ch <= '9')
    *u |= ch - '0';
else if (ch >= 'A' && ch <= 'F')
    *u |= ch - ('A' - 10);
else if (ch >= 'a' && ch <= 'f')
    *u |= ch - ('a' - 10);
else
    return nullptr;
```

然后，它检查字符 `ch` 是否是有效的十六进制数字。如果是有效的十六进制数字（0-9 或 A-F 或 a-f），它将该数字添加到码点 `u` 中。例如，如果字符 `ch` 是 '7'，则将十进制的 7 添加到码点 `u` 中。如果字符 `ch` 是 'A'，则将十进制的 10 添加到码点 `u` 中。如果字符不是有效的十六进制数字，它会返回 `nullptr`，表示解析失败。

最后，如果成功解析了 4 个有效的十六进制数字，并将它们转换为码点 `u`，则返回 JSON 字符串中的下一个位置（即指向下一个字符）。如果解析失败，函数将返回 `nullptr`。

接下来是Unicode编码函数：

```
static void lept_encode_utf8(lept_context* c,unsigned u)
{
    if(u<0x7F)
        PUTC(c,u&0xFF);
    else if(u<=u&0x7FF)
    {
        PUTC(c,0xC0|((u>>6)&0xFF));
        PUTC(c,0x80|(u&0x3F));
    }
    else if(u<=0xFFFF)
    {
        PUTC(c,0xE0|((u>>12)&0xFF));
        PUTC(c,0x80|((u>>6)&0x3F));
        PUTC(c,0x80|(u&0x3F));
    }
    else
    {
        assert(u<=0x10FFFF);
        PUTC(c,0xF0|((u>>18)&0xFF));
        PUTC(c,0x80|((u>>12)&0x3F));
        PUTC(c,0x80|((u>>6)&0x3F));
        PUTC(c,0x80|(u&0x3F));
    }
}
```

1. `if (u < 0x7F)`：首先检查字符 `u` 是否在 7 位 ASCII 范围内。如果是，那么它可以直接用 1 个字节的 UTF-8 编码表示。此时，`u` 的二进制值直接写入到解析上下文 `c` 中，通过 `PUTC(c, u & 0xFF)` 来实现。
2. `else if (u <= 0x7FF)`：如果字符 `u` 在 7 位 ASCII 范围之外，但在 11 位 Unicode 范围内，那么需要使用 2 个字节的 UTF-8 编码。在这种情况下，编码方式为 `110xxxxx 10xxxxxx`。具体的处理方式是将 `u` 拆分为两个字节，并用 `0xC0` 和 `0x80` 来设置 UTF-8 编码的头部和尾部。
3. `else if (u <= 0xFFFF)`：如果字符 `u` 在 11 位 Unicode 范围之外，但在 16 位 Unicode 范围内，那么需要使用 3 个字节的 UTF-8 编码。编码方式为 `1110xxxx 10xxxxxx 10xxxxxx`。这里将 `u` 拆分为三个字节，分别用 `0xE0` 和两个 `0x80` 来设置 UTF-8 编码的头部和尾部。
4. `else`：最后，如果字符 `u` 在 16 位 Unicode 范围之外，那么需要使用 4 个字节的 UTF-8 编码。编码方式为 `11110xxx 10xxxxxx 10xxxxxx 10xxxxxx`。在这种情况下，将 `u` 拆分为四个字节，使用 `0xF0` 和三个 `0x80` 分别设置 UTF-8 编码的头部和尾部。

解释下右移的操作：

右移18位出现在处理4字节UTF-8编码的情况，也就是处理那些Unicode码点大于0xFFFF的字符。这是因为在UTF-8编码中，一个4字节的字符被分成了4个部分来存储，每个部分占6位，加上每个字节前面的标志位，正好是4个字节。

举个例子，假设我们有一个Unicode码点是0x10FFFF，这是Unicode能表示的最大值。我们想要将它转换为UTF-8编码。首先，我们需要将这个值分成4个部分，每个部分6位。所以我们需要将这个值右移18位，12位和6位来获得每个部分的值。

具体来说：

1. `0xF0|((u>>18)&0xFF)`：这里`u>>18`将u右移18位，取最高的6位，然后通过`|0xF0`设置UTF-8编码的高位标志。
2. `0x80|((u>>12)&0x3F)`：这里`u>>12`将u右移12位，取次高的6位，然后通过`|0x80`设置UTF-8编码的高位标志。
3. `0x80|((u>>6)&0x3F)`：这里`u>>6`将u右移6位，取次低的6位，然后通过`|0x80`设置UTF-8编码的高位标志。
4. `0x80|(u&0x3F)`：这里直接取最低的6位，然后通过`|0x80`设置UTF-8编码的高位标志。

接下来是一个具体的例子：

假设我们有一个Unicode码点是0x10FFFF，也就是二进制的`100001111111111111111`。我们想要将它转换为UTF-8编码。首先，我们需要将这个值分成4个部分，每个部分6位。

具体来说：

1. `0xF0|((u>>18)&0xFF)`：这里`u>>18`将u右移18位，取最高的6位，也就是`100001`，然后通过`|0xF0`设置UTF-8编码的高位标志，得到`11110000 10000100`。
2. `0x80|((u>>12)&0x3F)`：这里`u>>12`将u右移12位，取次高的6位，也就是`111111`，然后通过`|0x80`设置UTF-8编码的高位标志，得到`10000000 11111100`。
3. `0x80|((u>>6)&0x3F)`：这里`u>>6`将u右移6位，取次低的6位，也就是`111111`，然后通过`|0x80`设置UTF-8编码的高位标志，得到`10000000 11111100`。
4. `0x80|(u&0x3F)`：这里直接取最低的6位，也就是`111111`，然后通过`|0x80`设置UTF-8编码的高位标志，得到`10000000 11111100`。

所以对于0x10FFFF这个Unicode码点，它的UTF-8编码就是

```
11110000 10000100
10000000 11111100
10000000 11111100
10000000 11111100
```

上面的100001被扩充为10000100而不是00100001，这是由于UTF8中用最高位标识一个编码的开始还是中间，我们取得的是最高6位，当然在低位补零。