# 文件系统发展史

## 本地文件服务器

**特点**：本地文件服务器是指**文件数据直接存储在本地节点**中。比如直接在项目目录下建立文件夹存放项目文件资源，如果按不同类型再细分，可以在项目目录下继续创建不同的子目录用于区分。

**优点**：简单便捷，项目可以直接引用，访问方便。

**缺点**：文件与代码混合存储不便于管理，随着文件的增多影响项目发布上线周期。

## 独立文件服务器

**特点**：搭建一台独立的服务器用于文件存储使用，项目上传文件时，先通过 ftp 或者 ssh 将文件上传至服务器某个目录下，再通过 Ngnix 或者 Apache Http Server 反向代理此目录，返回一个独立域名的文件 URL 地址，前端通过这个 URL 地址即可直接访问文件。

**优点**：独立存储，可以方便扩容、容灾和数据迁移。方便做图片访问请求的负载均衡，方便应用各种缓存策略（HTTP Header、Proxy Cache 等），也更加方便迁移到 CDN。（CDN是内容分发网络（Content Delivery Network）的缩写。它是一种网络架构，旨在提高互联网上内容（如网页、图像、视频等）的传输速度和性能，以便用户可以更快地访问这些内容。CDN技术通过将内容分发到多个地理位置的服务器上，使内容更接近最终用户，从而减少了延迟和提高了响应时间。）而且图片访问是很消耗服务器资源的（因为会涉及到操作系统的上下文切换和磁盘 I/O 操作），分离出来以后，Web/App 服务器可以更专注发挥动态处理的能力。

**缺点**：单机存在性能瓶颈，容灾、垂直扩展性差。

## 分布式文件服务器

**特点**：分布式文件系统一般包括**访问的仲裁**，**文件的存储**，**文件的容灾**三大块。仲裁模块相当于文件服务器的大脑，根据一定的算法来决定文件存储的位置。文件存储模块负责保存文件。容灾模块负责文件数据的相互备份。

**优点**：弹性伸缩，性能优越，扩展性强，可靠性高。

**缺点**：系统复杂度稍高，需要更多服务器。

# 文件系统发类别

## 单机文件系统

**特点**：用于操作系统和应用程序的本地存储。

**缺点**：数据无法在多台机器之间共享。

**代表**：EXT2、EXT3、EXT4、NTFS、FAT、FAT32、XFS、JFS 等等。

![tradition_file_system](https://github.com/sfssa/cpp-library/blob/master/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%8D%E5%8A%A1%E5%99%A8/FastFDS/static/tradition_file_system.png)

## 网络文件系统

**特点**：基于现有以太网架构，实现不同服务器之间传统文件系统的数据共享。

**缺点**：两台服务器不能同时访问修改，性能有限。

**代表**：NFS、CIFS 等等，比如下图 Windows 主机之间进行网络文件共享就是通过微软公司自己的 CIFS 服务实现的。

## 分布式文件系统

​	数据量越来越多，在一个操作系统管辖的范围存不下了，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，因此迫切需要一种系统来管理多台机器上的文件，**这就是分布式文件管理系统**。

　　分布式文件系统（Distributed File System）是一种**允许文件通过网络在多台主机上共享**的文件系统，可以让多机器上的多用户进行文件分享和存储。在这样的文件系统中，客户端并非直接访问底层的数据存储区块，而是通过网络，以特定的通信协议和服务器沟通。DFS 为分布在网络上任意位置的资源提供一个逻辑上的树形文件系统结构，让用户访问分布在网络上的共享文件更加简便。所有高层次的文件系统都是以低层次的传统文件系统为基础，实现了更高级的功能。

　　**特点**：在传统文件系统上，通过额外模块实现数据跨服务器分布，并且自身集成 RAID 保护功能，可以保证多台服务器同时访问、修改同一个文件系统。性能优越，扩展性强，可靠性高。

　　**缺点**：部分类型存在单点故障风险。

　　**代表**：HDFS（ASF）、MogileFS（LiveJournal）、FastDFS（余庆）、Lustre（Oracle）、GlusterFS（RedHat）等等。

![distribute_file_system](https://github.com/sfssa/cpp-library/blob/master/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%8D%E5%8A%A1%E5%99%A8/FastFDS/static/distribute_file_system.png)

### 通用型

通用分布式文件系统和传统的本地文件系统（如 EXT4、NTFS 等）相对应。典型代表：Lustre、MooseFS。

　　**优点**：传统文件系统的操作方式，对开发者门槛较低。

　　**缺点**：系统复杂性较高，需要支持若干标准的文件操作，如：目录结构、文件读写权限、文件锁等。系统整体性能有所降低，因为要支持 POSIX 标准（可移植操作系统接口 Portable Operating System Interface of UNIX）。

### 专用型

专用分布式文件系统基于谷歌文件系统论文（Google File System）的设计思想而来，文件上传后不能修改。使用专有 API 对文件进行访问，也可称为分布式文件存储服务。典型代表：HDFS、MogileFS、FastDFS。

　　**优点**：系统复杂性较低，不需要支持若干标准的文件操作，如：目录结构、文件读写权限、文件锁等。系统整体性能较高，因为无需支持 POSIX 标准，系统更加高效。

　　**缺点**：采用专有 API 对文件进行访问，对开发者门槛较高，一般都是**直接封装成工具类**进行使用。

# 什么是文件系统

​	文件系统（File System）是一种用于组织和管理计算机上的数据的方法，它定义了文件和目录的结构，以及如何存储、访问和维护这些数据。文件系统是操作系统的一部分，负责管理文件和数据的物理存储、访问权限、文件的元数据（例如文件名、大小、创建日期、修改日期等），以及文件的组织结构（目录结构）。

![FileSystem](https://github.com/sfssa/cpp-library/blob/master/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%8D%E5%8A%A1%E5%99%A8/FastFDS/static/FileSystem.png)

## 文件系统的基本组成

​	文件系统是操作系统中负责管理持久数据的子系统，简单来说，就是负责把用户的文件保存到磁盘上，磁盘上的文件并不会像内存中的数据那样在断电后数据丢失，磁盘中的文件不会发生丢失，因此可以持久化的保持文件。

​	文件系统的基本数据单位是文件，目的是对磁盘上的文件进行组织管理，在不同的操作系统上有不同的组织方式，这就会形成不同的文件系统。

​	Linux中很经典的一句话是：一切皆文件！，不仅普通的文件和目录，就连块设备，管道，socket等都是文件，拥有不同的访问权限。这些都是通过文件系统来管理。Linux会为每个文件分配两个数据结构：索引节点和目录项，用来记录文件的元信息和目录层次结构。

- 索引节点：inode，记录文件的元信息。比如inode编号、文件大小、访问权限、创建时间，最后一次修改时间、在磁盘中的位置等。**索引节点是文件的唯一标识，一一对应，因此索引节点也占用磁盘空间。**
- 目录项：dentry，用来记录文件的名字、索引节点指针以及和目录项层级关联关系。多个目录项关联在一起，就会形成目录结构。与索引节点不同的是，**目录项是由内核维护的一个数据结构，不存放于磁盘，而是缓存在内存。**

![directory](E:\projects_code\wokedir\cpp_library\分布式服务器\static\directory.png)

根据索引节点和目录项的特点我们可以发现：一个目录项中可以包含多个索引节点，也就是说目录项和索引节点的关系是多对一，也就是一个文件可以被多个目录包含。比如，硬链接的实现就是多个目录项中的索引节点指向同一个文件。

这里简单的介绍下Linux系统下的软连接和硬链接：

1. **硬链接**：
   - 硬链接是多个文件系统入口（目录项）指向同一个物理数据块（inode）的链接。这意味着硬链接的多个实例实际上都指向相同的数据。
   - **删除原始文件不会影响硬链接的可用性，因为数据仍然存在于磁盘上，只有当所有硬链接都被删除时，数据才会被释放。**
   - 硬链接通常只能在同一文件系统内创建，因为它们依赖于inode编号。如果跨越文件系统，inode编号将不同。
   - **硬链接不支持链接目录。**
2. **软链接**：
   - 软链接是一个特殊的文件，包含对另一个文件或目录的路径的引用。这个路径可以跨越文件系统边界。
   - **如果原始文件被删除，软链接将成为坏链接（broken link），因为它引用的文件不存在。**
   - **软链接可以指向目录**，也可以用于创建跨文件系统的链接。

目录也是文件，也用索引节点唯一标识，和普通文件不同的是，普通文件在磁盘里面保存的是文件数据，而目录文件在磁盘里面保存子目录或文件。

索引节点是存储在硬盘上的数据，为了快速访问文件，通常将索引节点加载到内存。此外，磁盘格式化的时候，会被分成三个存储区域，分别是超级块、索引节点和数据块区。

- 超级块：存储文件系统的详细信息，比如块个数、块大小、空闲块等。
- 索引节点区：存储索引节点。
- 数据块区：存储文件或目录数据。

我们并不会把超级块和索引节点区全部加载到内存， 这样内存撑不住，所以只有当需要使用时才将其加载到内存。其中，超级块在文件系统挂载时进入内存；索引节点区当文件被访问时进入内存。

### 目录项和目录是一个概念吗？

​	两者名字很相近，但是实际差别很大。首先，**目录是一个文件，持久化的存储在磁盘上。而目录项是内核的一个数据结构，存储在内存上。**

​	如果查询目录频繁的从磁盘读，效率很低，所以内核会把已经度过的目录用目录项来缓存在内存中，对内存操作的速度远远高于对磁盘的操作，下次再次读取到相同的目录时，只需要从内存读取目录项，然后再去访问磁盘，可以缩小所需要消耗的时间，提高了文件系统的效率（目录项不只是可以标识目录，也可以表示文件）。

### 文件是如何存储在磁盘中的呢？

​	磁盘读写的最小单位是磁盘块，磁盘块的大小通常是512字节，如果每次读取512字节会导致频繁的访问磁盘，这会导致读写的效率非常低。因此，文件系统把多个磁盘块组成了一个逻辑块，每次读写的最小单位是逻辑块（数据块），Linux中的逻辑块大小为4KB，也就是一次读取8个磁盘块，这会大大提高磁盘的读写效率。

### 为什么内存处理比磁盘IO快？

​	磁盘的物理特性导致它读写速度相对较慢，我们已经知道，磁盘读写的最小单位是磁盘块，确定一个磁盘块需要磁道号和扇区号，定位一个磁盘块需要物理移动机械臂和磁头；而内存中的数据可以直接通过电路访问。内存的读取通常比硬盘快几个数量级。

## 虚拟文件系统

​	虽然有多种文件系统，但是操作系统希望对用户提供一个统一的接口，用于在用户层与文件系统层引入了中间层，这个中间层就是虚拟文件系统。(Virtual File System,VFS)。

​	VFS定义了一组所有文件系统都支持的数据结构和标准接口，这样程序员不需要了解文件系统的工作原理，只需要了解VFS提供的统一接口即可。

​	在Linux文件系统中，用户空间、系统调用、虚拟机文件系统、缓存、文件系统以及存储之间的关系入下图所示：

![VFS](E:\projects_code\wokedir\cpp_library\分布式服务器\static\VFS.png)

​	Linux支持的文件系统有很多，根据存储位置的不同，可以将文件系统分为三类：

- 磁盘的文件系统，直接把数据存储在磁盘中，比如Ext/2/3/4，XFS等都是这类文件系统；
- 内存的文件系统，这类文件系统的数据不是存储在硬盘的，而是占用内存空间，常用到的/proc/sys文件系统都属于这一类，读写这类文件，实际上时读写内核中相关的数据结构；
- 网络的文件系统，用来访问其他计算机主机数据的文件系统，比如NFS、SMB等待。

文件系统首先要挂载到某个目录才可以正常使用（一切皆文件），比如Linux在启动时，将文件系统挂载到根目录上。

## 文件的使用

![file_read_write](E:\projects_code\wokedir\cpp_library\分布式服务器\static\file_read_write.png)

```c
fd = open(name, flag); # 打开文件
...
write(fd,...);         # 写数据
...
close(fd);             # 关闭文件
```

上面的代码模拟了读写文件的过程：

- 用open函数系统调用打开文件，open参数是包含文件的路径和文件名；
- 使用write写数据，通过文件描述符来标识文件；
- 使用close系统调用关闭文件，避免资源的泄露。

​	当我们通过系统调用打开一个资源文件后，操作系统会跟踪进程打开的所有文件（这里不是线程，因为进程是资源分配的基本单位，线程是调度的基本单位）。所谓跟踪就是操作系统为每个进程维护一个打开文件或其他IO资源，文件描述符是非负整数，通常是从0开始递增的，其中0、1和2分别是标准输入、标准输出和标准错误。每个文件描述符在文件描述符表中有一个条目，该条目包含有关该文件描述符的信息。

​	当进程打开一个文件时，Linux内核为该文件分配一个文件描述符。该文件描述符是唯一的，用于标识该文件。进程可以通过文件描述符来进行读取、写入、定位和关闭文件，而无需知道文件的物理位置。

Linux通过文件描述符找到进程打开的文件的简要过程如下所示：

1. 进程在打开文件时，内核为该文件分配一个文件描述符，这个文件描述符在进程的文件描述符表中占据一个条目；
2. 每个文件描述符条目中包含了文件描述符的整数值，以及指向文件表的指针。文件表是内核数据结构，跟踪所有已打开的文件。文件表中的每个条目都包含了文件的状态信息，如文件位置、打开标志、文件引用计数等；
3. 进程执行读写或其他文件操作时，通过文件描述符找到相应的文件表条目；
4. 进程的系统调用会将文件描述符转换成文件表条目的指针，通过文件表条目的指针找到磁盘上的文件读取到内存，然后执行所需的文件操作；
5. 内核管理文件表中的文件，包括维护文件的位置、处理并发访问、更新文件的状态等。

​	操作系统在打开文件表中维护着打开文件的状态信息：

- 文件指针：系统跟踪上次读写位置作为当前文件位置指针，这种指针对打开文件的某个进程来说是唯一的；
- 文件打开计数器：文件关闭时，操作系统必须重用其打开文件表条目，否则表内部空间不够用。因为多个进程可能同时打开同一个文件，因此系统在删除打开文件条目之前，必须等最后一个进程关闭文件，该计数器跟踪打开和关闭的数量，当计数为0时，系统关闭文件并删除该条目；
- 文件磁盘位置：绝大多数文件操作都要求系统修改文件数据，该信息保存在内存中，以免每个操作都从磁盘读取，降低系统的效率；
- 访问权限：每个进程打开文件都需要有一个访问模式（创建、只读、读写、添加等），该信息保存在进程的打开文件表中，以便操作系统能允许或拒绝之后的IO请求。

​	不难看出，用户和操作系统对文件的读写操作是有差异的，用户习惯以字节的方式读写文件，而操作系统是以数据块来读写文件，而文件系统的功能之一就是屏蔽掉这种差异。

### 读文件和写文件的过程

- 读文件：用户进程从文件读取一个字节大小的数据时，文件系统需要获取该字节数据所在的数据块，然后将其读取到内存，返回用户进程所需的数据部分；
- 写文件：用户把一个字节大小的数据写文件时，文件系统找到需要写入数据的数据块的位置，然后修改数据块中对应的位置，最后把数据块写回磁盘。

### 数据块、磁盘块和逻辑块的区别

1. **数据块（Data Block）：**
   - 数据块是文件系统中的一个基本单位，用于存储文件的实际数据内容。
   - 它是存储介质上的物理块，通常对应于磁盘上的一个扇区（sector）或一组扇区。
   - 数据块的大小是固定的，通常在文件系统创建时就确定了，例如 4KB 或 8KB。
   - 连续分配是一种分配数据块的方式，它要求分配连续的数据块来存储文件的连续数据。
2. **磁盘块（Disk Block）：**
   - 磁盘块是存储介质上的物理块，通常对应于磁盘上的一个扇区。
   - 磁盘块的大小是存储设备的物理特性，通常为 512 字节或 4KB。
   - 文件系统将数据块映射到磁盘块，以便实际在磁盘上存储和检索数据。
3. **逻辑块（Logical Block）：**
   - 逻辑块是文件系统内部使用的一个单位，用于管理文件系统的元数据和内部结构。
   - 逻辑块的大小和定义取决于文件系统的设计，通常大于磁盘块的大小。
   - 逻辑块用于管理文件分配表、目录结构、缓存等内部数据结构。
   - 文件系统可以将多个磁盘块映射到一个逻辑块，以提高性能和管理效率。

其中某些文件系统中逻辑块和数据块概念是通用的。

## 文件的存储

​	文件的数据主要是存放在磁盘上的，只有在需要处理时才会将其读取到内存中，数据在磁盘上的存放方式，就像程序在内存中存放的方式：

- 连续空间存放方式；
- 非连续空间存放方式。

其中，非连续空间存放方式可以分为链表方式和索引方式。不同的存储方式有各自的特点和适应场景，重点是分析它们的存储效率和读写性能。

### 连续空间存放方式

​	连续空间存放方式中，文件存放在磁盘连续的物理空间中。这种模式下，数据紧密相连，读写效率高，一次磁盘寻道读取整个文件。但是使用连续存放的方式有一个前提，就是实现知道文件的大小，然后根据文件的大小在磁盘上找到一块连续的空间分配给文件。

​	因此，文件头里需要指定起始块的位置和长度，通过这两个信息可以表示文件存放方式是连续空间。（文件头类似于上文的inode）

![file_storage](E:\projects_code\wokedir\cpp_library\分布式服务器\static\file_storage.png)

​	连续存放的方式读写效率很高，支持随机访问，但是有磁盘空间碎片和文件长度不易扩展的缺陷。如下图所示：文件B被删除，磁盘上留下一块空缺，这时如果新来的文件小于其中一个空缺，我们可以将其放在相应空缺里。但如果该文件的大小大于所有的空缺，但是小于所有空缺空间之和，这就好导致虽然磁盘有足够的空间存放新的文件，但是却无法存放。通常操作系统提供了内存重组来重新调整文件所在磁盘位置来腾出新的空间来容纳新的文件，但是这个过程非常耗时，严重降低计算机的性能：

1. **紧凑内存分配（Memory Compaction）：** 这种方法通过将已分配的内存块移到一起来减少外部碎片。这通常需要操作系统支持，因为它需要重新分配内存中的数据。
2. **内存重分配（Memory Relocation）：** 在某些情况下，操作系统可以选择重新分配内存块，以便将它们放置在连续的地址空间中，从而减少碎片。
3. **动态分配策略（Dynamic Allocation Strategies）：** 使用更智能的内存分配策略，可以在一定程度上减少内部碎片。例如，使用分区分配或伙伴系统来减少内存浪费。

![shard](E:\projects_code\wokedir\cpp_library\分布式服务器\static\shard.png)

​	还有一个问题就是不方便扩充，比如原来是100个字节的空间，结果文件现在被修改导致需要200个字节的空间，一个常用的解决办法是先找到另一块容量是200字节的空间，然后将原来的数据复制过去，之后将新数据插入到新的空间中，最后释放原来的空间，这也会降低计算机的性能。

### 非连续空间存储方式

​	非连续空间的存放方式是离散的、不需要连续空间的，在这种情况下可以消除磁盘碎片，大大提高磁盘空间的利用率，同时文件的长度可以非常方便的动态扩展

#### 链表

	##### 隐式链表

​	实现方式是文件头包含文件第一块和最后一块位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置。这样一个数据块连着下一个数据块，从链表头可以顺着指针找到所有的数据块，存放的空间可以是不连续的。除文件的最后一个盘块之外，每个盘块都存有指向下一个盘块的指针

![link_view](E:\projects_code\wokedir\cpp_library\分布式服务器\static\link_unview.png)

文件目录中记录存放该文件在外存中的**起始块号**和**结束块号**

用户给出要访问的逻辑块号，操作系统找到该文件对应的目录项（FCB）

- 从目录项中找到起始块号（即 0 号块），将 0 号逻辑地址读入内存，由此知道 1 号逻辑块存放的物理地址，于是读入 1 号逻辑块，再找到 2 号逻辑块，以此类推...
- 因此，读入 i 号逻辑块，总共需要 i + 1 次磁盘 I/O

**扩展文件：**随便找到一个空闲磁盘块，挂到文件的磁盘块链尾，并修改文件的 FCB

**优点：**方便文件扩展，不会有碎片问题，外存利用率高

**缺点：**只支持顺序访问，不支持随机访问，查找效率低，指向下一个磁盘块的指针也需要消耗一定的存储空间

##### 显示链表

​	把用于链接文件各物理块的指针显式地存放在一张表中，即**文件分配表（FAT）**。假设某个新创建的文件 “aaa” 依次存放在磁盘块 2 -> 5 -> 0 -> 1；假设某个新创建的文件 “bbb” 依次存放在磁盘块4 -> 23 -> 3。其文件分配表如下图所示：

![link_unview](E:\projects_code\wokedir\cpp_library\分布式服务器\static\link_view.png)

**注意：一个磁盘仅设置一张 FAT。开机时，将 FAT 读入内存，并常驻内存。**FAT 的各个表项在物理上连续存储，且每一个表项长度相同，因此，物理块号可以隐含

用户给出要访问的逻辑块号，操作系统找到该文件对应的目录项（FCB）

- 从目录项中找到起始块号，若 i > 0，则查询内存中的文件分配表 FAT，往后找到 i 号逻辑块对应的物理块号。逻辑块号转换成物理块号的过程不需要读磁盘操作

**优点：**很方便文件拓展，不会有碎片问题，外存利用率高，并且**支持随机访问**。相比于隐式链接来说，**地址转换时不需要访问磁盘，因此文件的访问效率更高**

**缺点：**文件分配表的需要占用一定的存储空间

#### 索引

**索引分配**允许文件离散地分配在各个磁盘块中，系统会**为每个文件建立一张索引表**，索引表中**记录了文件的各个逻辑块对应的物理块**（索引表的功能类似于内存管理中的页表——建立逻辑页面到物理页之间的映射关系）。索引表存放的磁盘块称为**索引块**，文件数据存放的磁盘块称为**数据块**

![index](E:\projects_code\wokedir\cpp_library\分布式服务器\static\index.png)

##### 链接索引

​	如果一个文件的大小超过了256块，那么一个磁盘块是装不下文件的整张索引表的，如何解决这个问题？

​	如果索引表太大，一个索引块装不下，那么可以将多个索引块链接起来存放

![link_index](E:\projects_code\wokedir\cpp_library\分布式服务器\static\link_index.png)

**存在问题：**如果需要访问第 256 块，需要先调出 7 号物理块对应的索引块，然后遍历到 255 号，知道第二个索引块，然后才能访问到 256 号，效率极低

##### 多层索引

建立多层索引（**原理类似于多级页表**）。使第一层索引块指向第二层的索引块。还可根据 文件大小的要求再建立第三层、第四层索引块

![indexs](E:\projects_code\wokedir\cpp_library\分布式服务器\static\indexs.png)

假设磁盘块大小为 1KB，一个索引表项占 4B，则一个磁盘块只能存放 256 个索引

若某文件采用**两层索引**，则该**文件最大长度**可以到 256 * 256 * 1KB = 65536KB = 64MB

可以根据逻辑块号计算出应该查询索引表中的表项：

- 如：要访问 1026 号逻辑块，则 1026 / 256 = 4，1026 % 256 = 2
- 一级索引的 4 号块，二级索引的 2 号块

采用 K 层索引结构，且**顶级索引表未调入内存**，则访问一个数据块只需要 K + 1 次读写操作

##### 混合索引

​	多种索引方式的结合。例如，一个文件的顶级索引表中，既包含**直接地址索引**（直接指向**数据块**），又包含**一级间接索引**（指向单层索引块），还包含**两级间接索引**（指向两层索引表）。

![mix_index](E:\projects_code\wokedir\cpp_library\分布式服务器\static\mix_index.png)

其中Unix的文件存储方式就是同时组合了文件存放的优点，如下图所示：

![unix](E:\projects_code\wokedir\cpp_library\分布式服务器\static\unix.png)

它是根据文件的大小，存放的方式会有所变化：

- 如果存放文件所需的数据块小于 10 块，则采用直接查找的方式；
- 如果存放文件所需的数据块超过 10 块，则采用一级间接索引方式；
- 如果前面两种方式都不够存放大文件，则采用二级间接索引方式；
- 如果二级间接索引也不够存放大文件，这采用三级间接索引方式；

那么，文件头（*Inode*）就需要包含 13 个指针：

	 - 10 个指向数据块的指针； 
	 -  第 11 个指向索引块的指针； 
	 -  第 12 个指向二级索引块的指针；
	 -  第 13 个指向三级索引块的指针；

所以，这种方式能很灵活地支持小文件和大文件的存放： -

- 对于小文件使用直接查找的方式可减少索引数据块的开销； -
- 对于大文件则以多级索引的方式来支持，所以大文件在访问数据块时需要大量查询；

这个方案就用在了 Linux Ext 2/3 文件系统里，虽然解决大文件的存储，但是对于大文件的访问，需要大量的查询，效率比较低。

### 总结

若文件太大，索引表项太多，可以采取以下三种方法解决：

**链接方案：**

- 如果索引表太大，一个索引块装不下，那么可以将多个索引块链接起来存放
- **缺点：**若文件很大，索引表很长，就需要将很多个索引块链接起来。想要找到 i 号索引块，必须先依次读入0 ~ i - 1 号索引块，这就导致磁盘 I/O 次数过多，查找效率低下

**多层索引：**

- 建立多层索引（原理类似于多级页表）。使第一层索引块指向第二层的索引块。还可根据文件大小的要求再建立第三层、第四层索引块。采用K 层索引结构，且顶级索引表未调入内存，则访问一个数据块只需要K + 1 次读磁盘操作
- **缺点：**即使是小文件，访问一个数据块依然需要K+1次读磁盘

**混合索引：**

- 多种索引分配方式的结合。例如，一个文件的顶级索引表中，既包含直接地址索引（直接指向数据块），又包含一级间接索引（指向单层索引表）、还包含两级间接索引（指向两层索引表）
- **优点：**对于小文件来说，访问一个数据块所需的读磁盘次数更少

![sumary](E:\projects_code\wokedir\cpp_library\分布式服务器\static\sumary.png)

## 空闲空间存储

​	接下来介绍，如果我要保存一个数据块，我应该将数据块放到磁盘上哪个位置？

#### 空闲表法

​	空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图：

![free_block](E:\projects_code\wokedir\cpp_library\分布式服务器\static\free_block.png)

​	当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号和它占用的块数填充到这个条目中。**这种方法在空闲区少时有较好的效果，如果存储空间中有大量的小的空闲区，那么空闲链表变得很大，这样查询效率很低。**另外，这种分配技术适合建立连续文件。

#### 空闲链表法

​	我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。如下图：

![free_blocks_tables](E:\projects_code\wokedir\cpp_library\分布式服务器\static\free_blocks_tables.png)

当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。

这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。

**空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。**

1. **空间开销：** 在大型文件系统中，磁盘容量通常非常庞大，包含大量的簇或块。使用空闲表法或空闲链表法来跟踪每个簇或块的状态和分配情况会占用大量的额外磁盘空间。对于大容量磁盘，这种额外空间开销可能会变得不可接受。
2. **性能问题：** 空闲表法和空闲链表法需要维护一个数据结构，以存储所有的空闲簇或块信息。在大型文件系统中，这个数据结构的维护和搜索操作可能会变得非常耗时，导致磁盘分配和释放的性能下降。
3. **复杂性：** 随着文件系统的增长，空闲表或链表的复杂性也会增加。这可能需要更多的内存和处理时间来管理这些数据结构，特别是在进行空间回收或碎片整理时。
4. **有限性：** 空闲表法和空闲链表法的设计通常基于文件系统容量的预估，如果文件系统超出了设计容量，可能会导致空间分配问题。这会使文件系统更加脆弱，难以扩展。

#### 位图法

​	位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。

当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：

```text
1111110011111110001110110111111100111 ...
```

在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。

## 文件系统的结构

​	前面提到的Linux是用位图的方式管理空闲空间的，用户在创建新文件时，Linux通过内核中的inode位图找到空闲可用的inode并进行分配。存储数据时，通过块的位图找到空闲的块并分配。那么现在有一个问题：数据块的位图放在磁盘块中，一个数据块假设是4K，每位表示一个数据块，可以表示`4*1024*8=2^15`个空闲块（1字节=8位），由于1个数据块是4K大小，那么最大可表示的空间是：`2^15*4*8=2^27`字节，也就是128MB。

​	**也就是说，采用一个块的位图+一系列的块，外加一个块的inode的位图+一系列的inode的结构最大能表示的空间只有128MB。**在Linux文件系统中，把这个结构称为一个块组，那么有N多的块组，就能表示N大的文件。下图给出了 Linux Ext2 整个文件系统的结构和块组的内容，文件系统都由大量块组组成，在硬盘上相继排布：

![ext2](E:\projects_code\wokedir\cpp_library\分布式服务器\static\ext2.png)

最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下： 

-  **超级块**，包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。 
- **块组描述符**，包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。 
-  **数据位图和 inode 位图**， 用于表示对应的数据块或 inode 是空闲的，还是被使用中。 
- **inode 列表**，包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。 
- *数据块*，包含文件的有用数据。

​	你可以会发现每个块组里有很多重复的信息，比如**超级块和块组描述符表，这两个都是全局信息，而且非常的重要**，这么做是有两个原因： - 如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。 - 通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。

不过，Ext2 的后续版本采用了稀疏技术。该做法是，超级块和块组描述符表不再存储到文件系统的每个块组中，而是只写入到块组 0、块组 1 和其他 ID 可以表示为 3、 5、7 的幂的块组中。

## 目录的存储

​	上面的内容都是文件的存储，那么目录该怎么存储呢？基于 Linux 一切皆文件的设计思想，目录其实也是个文件，你甚至可以通过 `vim` 打开它，它也有 inode，inode 里面也是指向一些块。和普通文件不同的是，**普通文件的块里面保存的是文件数据，而目录文件的块里面保存的是目录里面一项一项的文件信息。在目录文件的块中，最简单的保存格式就是列表，就是一项一项地将目录下的文件信息（如文件名、文件 inode、文件类型等）列在表里。列表中每一项就代表该目录下的文件的文件名和对应的 inode，通过这个 inode，就可以找到真正的文件。

![directory_file](E:\projects_code\wokedir\cpp_library\分布式服务器\static\directory_file.png)

​	通常，第一项是「`.`」，表示当前目录，第二项是「`..`」，表示上一级目录，接下来就是一项一项的文件名和 inode。如果一个目录有超级多的文件，我们要想在这个目录下找文件，按照列表一项一项的找，效率就不高了。

​	于是，保存目录的格式改成**哈希表**，对文件名进行哈希计算，把哈希值保存起来，如果我们要查找一个目录下面的文件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个文件的信息在相应的块里面。Linux 系统的 ext 文件系统就是采用了哈希表，来保存目录的内容，**这种方法的优点是查找非常迅速，插入和删除也较简单，不过需要一些预备措施来避免哈希冲突。**

​	目录查询是通过在磁盘上反复搜索完成，需要不断地进行 I/O 操作，开销较大。所以，**为了减少 I/O 操作，把当前使用的文件目录缓存在内存，以后要使用该文件时只要在内存中操作，从而降低了磁盘操作次数，提高了文件系统的访问速度。**

## 文件I/O

### 缓冲与非缓冲 I/O

​	文件操作的标准库是可以实现数据的缓存，那么**根据「是否利用标准库缓冲」，可以把文件 I/O 分为缓冲 I/O 和非缓冲 I/O**： 

- 缓冲 I/O，利用的是标准库的缓存实现文件的加速访问，而标准库再通过系统调用访问文件。 
- 非缓冲 I/O，直接通过系统调用访问文件，不经过标准库缓存。

这里所说的「缓冲」特指标准库内部实现的缓冲。

比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来，这样做的目的是，减少系统调用的次数，毕竟系统调用是有 CPU 上下文切换的开销的。

### 直接与非直接 I/O

我们都知道磁盘 I/O 是非常慢的，所以 Linux 内核为了减少磁盘 I/O 次数，在系统调用后，会把用户数据拷贝到内核中缓存起来，这个内核缓存空间也就是「页缓存」，只有当缓存满足某些条件的时候，才发起磁盘 I/O 的请求。

那么，**根据是「否利用操作系统的缓存」，可以把文件 I/O 分为直接 I/O 与非直接 I/O**：

- 直接 I/O，不会发生内核缓存和用户程序之间数据复制，而是直接经过文件系统访问磁盘。
- 非直接 I/O，读操作时，数据从内核缓存中拷贝给用户程序，写操作时，数据从用户程序拷贝给内核缓存，再由内核决定什么时候写入数据到磁盘。

如果你在使用文件操作类的系统调用函数时，指定了 `O_DIRECT` 标志，则表示使用直接 I/O。如果没有设置过，默认使用的是非直接 I/O。

如果用了非直接 I/O 进行写数据操作，内核什么情况下才会把缓存数据写入到磁盘？

​	以下几种场景会触发内核缓存的数据写入磁盘： 

-  在调用 `write` 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上； -
- 用户主动调用 `sync`，内核缓存会刷到磁盘上； -
- 当内存十分紧张，无法再分配页面时，也会把内核缓存的数据刷到磁盘上； -
- 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上；



### 阻塞与非阻塞 I/O VS 同步与异步 I/O

为什么把阻塞 / 非阻塞与同步与异步放一起说的呢？因为它们确实非常相似，也非常容易混淆，不过它们之间的关系还是有点微妙的。

先来看看**阻塞 I/O**，当用户程序执行 `read` ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，`read` 才会返回。

注意，**阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程**。过程如下图：

![direct_IO](E:\projects_code\wokedir\cpp_library\分布式服务器\static\direct_IO.png)

知道了阻塞 I/O ，来看看**非阻塞 I/O**，非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，`read` 调用才可以获取到结果。过程如下图：

![block_io](E:\projects_code\wokedir\cpp_library\分布式服务器\static\block_io.png)

注意，**这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。**

举个例子，访问管道或 socket 时，如果设置了 `O_NONBLOCK` 标志，那么就表示使用的是非阻塞 I/O 的方式访问，而不做任何设置的话，默认是阻塞 I/O。应用程序每次轮询内核的 I/O 是否准备好，感觉有点傻乎乎，因为轮询的过程中，应用程序啥也做不了，只是在循环，并且占用CPU资源。

为了解决这种傻乎乎轮询方式，于是 **I/O 多路复用**技术就出来了，如 select、poll，epoll，它是通过 I/O 事件分发，当内核数据准备好时，再以事件通知应用程序进行操作。

这个做法大大改善了应用进程对 CPU 的利用率，在没有被通知的情况下，应用进程可以使用 CPU 做其他的事情。

下图是使用 select I/O 多路复用过程。注意，`read` 获取数据的过程（数据从内核态拷贝到用户态的过程），也是一个**同步的过程**，需要等待：

![poll_select](E:\projects_code\wokedir\cpp_library\分布式服务器\static\poll_select.png)

实际上，无论是阻塞 I/O、非阻塞 I/O，还是基于非阻塞 I/O 的多路复用**都是同步调用。因为它们在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。**

而真正的**异步 I/O** 是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程都不用等待。

当我们发起 `aio_read` 之后，就立即返回，内核自动将数据从内核空间拷贝到应用程序空间，这个拷贝过程同样是异步的，内核自动完成的，和前面的同步操作不一样，应用程序并不需要主动发起拷贝动作。过程如下图：

![aio_read](E:\projects_code\wokedir\cpp_library\分布式服务器\static\aio_read.png)

在前面我们知道了，I/O 是分为两个过程的： 

1. 数据准备的过程 ；
2. 数据从内核空间拷贝到用户进程缓冲区的过程。

阻塞 I/O 会阻塞在「过程 1 」和「过程 2」，而非阻塞 I/O 和基于非阻塞 I/O 的多路复用只会阻塞在「过程 2」，所以这三个都可以认为是同步 I/O。异步 I/O 则不同，「过程 1 」和「过程 2 」都不会阻塞。

举个你去饭堂吃饭的例子，你好比用户程序，饭堂好比操作系统。

阻塞 I/O 好比，你去饭堂吃饭，但是饭堂的菜还没做好，然后你就一直在那里等啊等，等了好长一段时间终于等到饭堂阿姨把菜端了出来（数据准备的过程），但是你还得继续等阿姨把菜（内核空间）打到你的饭盒里（用户空间），经历完这两个过程，你才可以离开。

非阻塞 I/O 好比，你去了饭堂，问阿姨菜做好了没有，阿姨告诉你没，你就离开了，过几十分钟，你又来饭堂问阿姨，阿姨说做好了，于是阿姨帮你把菜打到你的饭盒里，这个过程你是得等待的。

基于非阻塞的 I/O 多路复用好比，你去饭堂吃饭，发现有一排窗口，饭堂阿姨告诉你这些窗口都还没做好菜，等做好了再通知你，于是等啊等（`select` 调用中），过了一会阿姨通知你菜做好了，但是不知道哪个窗口的菜做好了，你自己看吧。于是你只能一个一个窗口去确认，后面发现 5 号窗口菜做好了，于是你让 5 号窗口的阿姨帮你打菜到饭盒里，这个打菜的过程你是要等待的，虽然时间不长。打完菜后，你自然就可以离开了。

异步 I/O 好比，你让饭堂阿姨将菜做好并把菜打到饭盒里后，把饭盒送到你面前，整个过程你都不需要任何等待。

# FastDFS简介

​	FastDFS（Fast Distributed File System）是一个开源的分布式文件系统，旨在存储和管理大规模文件数据。它具有高性能、可伸缩性和可靠性，适用于需要大规模文件存储和分发的应用程序，如图片、音视频等多媒体数据的存储与访问。

以下是FastDFS的一些关键特点和组成部分：

1. **分布式架构：** FastDFS采用了分布式架构，允许数据存储在多个服务器节点上。这提供了高可用性和容错性，因为数据可以冗余存储在不同的节点上。
2. **高性能：** FastDFS旨在提供高性能的文件存储和访问。它使用轻量级的协议和高效的数据传输方式，以快速上传和下载文件。
3. **轻量级协议：** FastDFS使用自己的轻量级协议进行通信，减少了网络开销。这使得它适用于需要高吞吐量和低延迟的应用程序。
4. **文件管理：** FastDFS提供了文件管理功能，包括文件上传、下载、删除、查找和元数据管理。这使得文件的存储和访问更加方便。
5. **易扩展性：** FastDFS支持水平扩展，允许用户根据需求增加服务器节点以处理更多数据。
6. **开源和社区支持：** FastDFS是一个开源项目，有一个积极的开发社区，为用户提供支持和不断改进系统。
7. **负载均衡：** FastDFS支持负载均衡，可以通过访问不同的存储节点来分布负载，确保系统的高可用性和性能。

​	FastDFS通常用于互联网服务提供商、内容分发网络（CDN）提供商、在线媒体存储和访问等需要大规模文件存储的场景。它可以与其他存储解决方案（如Nginx、NFS等）集成，以满足不同应用程序的需求。

## FastDFS架构

![fastdfs_framework](E:\projects_code\wokedir\cpp_library\分布式服务器\static\fastdfs_framework.jpg)

## Tracker Server

​	**跟踪服务器**，负责文件访问的调度和负载均衡，负责管理所有的 Storage Server 和 group 组/卷。

## Storage Server

​	**存储服务器**，负责文件存储，文件同步/备份，提供文件访问接口，文件元数据管理。以 group 为单位，每个 group 内可以有多台 Storage Server，数据互为备份，达到容灾的目的。每个 Storage 在启动以后会主动连接 Tracker，告知自己所属 group 等存储相关信息，并保持周期性心跳。

## Client

​	**客户端**，实现文件上传下载的服务器，就是我们自己的项目所部署在的服务器。通过专有接口，使用 TCP/IP 协议与跟踪服务器或存储服务器进行数据交互。FastDFS 向使用者提供基本文件访问接口，比如 upload、download、append、delete 等，以客户端库的方式提供给用户使用。

上传

- 连接追踪器, 询问存储节点的信息
  - 我要上传1G的文件, 询问那个存储节点有足够的容量；
  - 追踪器查询, 得到结果
  - 追踪器将查到的存储节点的IP+端口发送给客户端
  - 通过得到IP和端口连接存储节点
  - 将文件内容发送给存储节点

下载

- 连接追踪器, 询问存储节点的信息
  - 问一下, 要下载的文件在哪一个存储节点
  - 追踪器查询, 得到结果
  - 追踪器将查到的存储节点的IP+端口发送给客户端
  - 通过得到IP和端口连接存储节点
  - 下载文件

## client、tracker、storage三者关系

上传：

![fdfs-file-upload](E:\projects_code\wokedir\cpp_library\分布式服务器\static\fdfs-file-upload.png)

下载：

![fdfs-file-down](E:\projects_code\wokedir\cpp_library\分布式服务器\static\fdfs-file-down.png)

## Group

​	**组**， 也可称为 Volume 卷。同组内服务器上的文件是完全相同的，同一组内的 Storage Server 之间是对等的，文件上传、删除等操作可以在任意一台 Storage Server 上进行。

​	**横向扩容：**增加Group；

​	**纵向扩容：**增加组内的主机数目；

​	**组容量：**每个组内容量最小的主机的容量；比如Group1：512G，1T，2T，组容量为512G；

​	**FastDFS总容量：**每个组的容量的累加和，比如Group1容量是512G，Group2容量是1T，总容量是：1T+512G。

## MetaData

​	文件系统中存储的数据分为**数据**和**元数据**两部分，数据是指文件中的实际数据，即文件的实际内容；而元数据是用来描述一个文件特征的系统数据，诸如访问权限、文件拥有者以及文件数据块的分布信息等等。如果文件是一张图片，元数据就是图片的宽，高等等。

## FastDFS存储策略

​	为了支持大容量存储，Storage 存储服务器采用了分组（或分卷）的方式。存储系统由一个或多个组组成，组与组之间的文件是相互独立的，所有组的文件容量累加就是整个存储系统中的文件容量。一个组可以由一台或多台存储服务器组成，一个组下的存储服务器中的文件都是相同的，组中的多台存储服务器起到了冗余备份和负载均衡的作用。

　　当组中增加了新的服务器时，系统会自动同步已有的文件，同步完成后，系统自动将新增的服务器切换至线上提供服务。

　　当存储空间不足时，可以动态添加组，只需要增加一台或多台服务器，并将它们配置为一个新的组，即可扩大存储系统的容量。当你的某个应用或者模块（对应的 group）的并发过高的时候，可以直接在 group 中增加若干个 Storage 来实现负载均衡。

　　为了避免单个目录下的文件数太多，当 Storage 第一次启动时，会在每个数据存储目录中创建 2 级子目录，每级 256 个，总共 65536 个目录，上传的文件会以 hash 的方式被路由到其中某个子目录下，然后将文件数据直接作为一个本地文件存储到该目录。

## FastDFS下载安装

安装包都放在了softwore目录下。

1. fastDFS安装

- libfastcommon-1.36.zip
  - fastdfs的基础库包
  - unzip xxx.zip
  - ./make.sh 
  - ./make.sh install（需要root用户）
- fastdfs-5.10.tar.gz
  - tar zxvf xxx.tar.gz
  - ./make.sh  
  - ./make.sh install

2. 测试

在`/usr/bin`目录中进行查找，执行命令：`ls -al /usr/bin/ | grep fdfs_`后显示如下：

![image-20231102202929528](C:\Users\16645\AppData\Roaming\Typora\typora-user-images\image-20231102202929528.png)

# 修改配置文件

**记住，log和文件上传的路径，该用户一定要对这个文件夹及内部的所有文件、文件夹有读写的权限！！！**

**记住，log和文件上传的路径，该用户一定要对这个文件夹及内部的所有文件、文件夹有读写的权限！！！**

**记住，log和文件上传的路径，该用户一定要对这个文件夹及内部的所有文件、文件夹有读写的权限！！！**

配置文件的默认位置是：`/etc/fdfs`，有下面四个文件：

`client.conf.sample  storage.conf.sample  storage_ids.conf.sample  tracker.conf.sample`

1. `client.conf.sample`: 这是 FastDFS 客户端的配置示例文件。 FastDFS 是一个开源的分布式文件系统，`client.conf.sample` 包含客户端配置选项的示例，用于配置客户端与 FastDFS 服务器通信的行为，例如服务器地址、连接池大小、存储节点选择策略等。
2. `storage.conf.sample`: 这是 FastDFS 存储服务器的配置示例文件。存储服务器是 FastDFS 系统中负责实际文件存储和检索的组件。`storage.conf.sample` 包含存储服务器配置选项的示例，例如存储路径、端口号、存储空间设置等。
3. `storage_ids.conf.sample`: FastDFS 存储服务器的 ID 配置示例文件。每个存储服务器都有一个唯一的标识符，`storage_ids.conf.sample` 包含示例配置，用于指定存储服务器的 ID。
4. `tracker.conf.sample`: 这是 FastDFS 跟踪器（Tracker）的配置示例文件。跟踪器是 FastDFS 系统中的协调组件，用于协调存储服务器和客户端之间的文件上传、下载等操作。`tracker.conf.sample` 包含跟踪器的配置选项示例。

在修改配置文件时复制原来的来进行修改。

## 配置并启动Tracker-第一个启动（守护进程）

```
# 复制原文件
cp tracker.conf.sample tracker.conf

# 修改下面的内容
# 将追踪器和部署的主机的IP地址进程绑定, 也可以不指定
# 如果不指定, 会自动绑定当前主机IP, 如果是云服务器建议不要写
bind_addr=192.168.126.132

# 追踪器监听的端口
port=22122

# 追踪器存储日志信息的目录, xxx.pid文件, 必须是一个存在的目录
base_path=/home/pzx/分布式服务器/fastfds

# 启动tracker
fdfs_trackerd /etc/fsfs/tracker.conf

# 关闭tracker
fdfs_trackerd /etc/fsfs/tracker.conf stop

# 重启tracker
fdfs_trackerd /etc/fsfs/tracker.conf restart
```

启动成功后，通过`ps aux | grep tracker`检验:

![tracker_status](E:\projects_code\wokedir\cpp_library\分布式服务器\static\tracker_status.png)

## 配置并启动Storage-第二个启动（守护进程）

```
# 当前存储节点对应的主机属于哪一个组
group_name=group1

# 当前存储节点和所应该的主机进行IP地址的绑定, 如果不写, 有fastdfs自动绑定
bind_addr=192.168.126.132

# 存储节点绑定的端口
port=23000

# 存储节点写log日志的路径
base_path=/home/pzx/分布式服务器/fastdfs

# 下面是提供两个存储文件路径
# 存储节点提供的存储文件的路径个数
# store_path_count=2
#
# 具体的存储路径
# base_path=/home/pzx/分布式服务器/fastdfs
# base_path=/home/pzx/分布式服务器/fastdfs1

# 本次只使用一个存储节点
storage_path_count=1
store_path0=/home/pzx/分布式服务器/fastdfs

# 追踪器的地址信息
tracker_server=192.168.126.132:22122 
# 可以有多个tracker来实现集群
# tracker_server=192.168.247.136:22122 

# 启动storage
fdfs_storaged /etc/fdfs/stroga.conf

# 停止storage
fdfs_storaged /etc/fdfs/stroga.conf stop

# 重启storage
fdfs_storaged /etc/fdfs/stroga.conf restart
```

![storage_list](E:\projects_code\wokedir\cpp_library\分布式服务器\static\storage_list.png)

## 修改客户端配置-最后启动（普通进程）

```
# 客户端写log日志的目录
# 该路径必须存在
# 当前的用户对于该路径中的文件有读写权限
# 当前用户robin
# 指定的路径属于root
base_path=/home/yuqing/fastdfs
# 要连接的追踪器的地址信息
tracker_server=192.168.247.135:22122 
tracker_server=192.168.247.136:22122 

# 上传
fdfs_upload_file /etc/fdfs/client.conf 要上传的文件
# 上传会得到一个字符串

# 下载
fdfs_download_file /etc/fdfs/client.conf 上传成功之后得到的字符串(fileID)
```

# 客户端操作

## 上传文件

```
# 下面一行是上传文件
fdfs_upload_file /etc/fdfs/client.conf main.cpp 
# 上传文件后会自动打印下面的字符串，也就是文件的唯一编码
group1/M00/00/00/wKh-hGVDnfyAP6IeAAAAYI8PpRg223.cpp
```

## file_id

```
group1/M00/00/00/wKh-hGVDnfyAP6IeAAAAYI8PpRg223.cpp
```

- group1
  - 文件上传到了存储节点的哪一个组
  - 如果有多个组这个组名可变的
- M00 - 虚拟目录
  - 和存储节点的配置项有映射
    - store_path0=/home/yuqing/fastdfs/data    ->  M00
      store_path1=/home/yuqing/fastdfs1/data   -> M01
- 00/00
  - 实际的路径
  - 可变的
- wKhS_VlrEfOAdIZyAAAJTOwCGr43848.md
  - 文件名包含的信息
  - 采用Base64编码
    - 包含的字段包括

      - 源storage server Ip 地址  
      - 文件创建时间  

      - 文件大小  

      - 文件CRC32效验码 

        - 循环冗余校验  

      - 随机数

为什么需要这个编码而不是原来的main.cpp呢？如果A上传一个main.cpp文件，B也上传一个main.cpp文件，两者出现了重名。这种情况下就无法确定A和B所需要的main文件是哪个。为了避免这种情况，通过Base64编码生成唯一的ID来保证不会重名。

## 下载文件

```
fdfs_download_file /etc/fdfs/client.conf group1/M00/00/00/wKh-hGVDnfyAP6IeAAAAYI8PpRg223.cpp
```

## 删除文件

```
fdfs_delete_file /etc/fdfs/client.conf group1/M00/00/00/wKh-hGVDnfyAP6IeAAAAYI8PpRg223.cpp
```

# 状态检测

```
fdfs_monitor /etc/fdfs/client.conf

# FDFS_STORAGE_STATUS：INIT      :初始化，尚未得到同步已有数据的源服务器
# FDFS_STORAGE_STATUS：WAIT_SYNC :等待同步，已得到同步已有数据的源服务器
# FDFS_STORAGE_STATUS：SYNCING   :同步中
# FDFS_STORAGE_STATUS：DELETED   :已删除，该服务器从本组中摘除
# FDFS_STORAGE_STATUS：OFFLINE   :离线
# FDFS_STORAGE_STATUS：ONLINE    :在线，尚不能提供服务
# FDFS_STORAGE_STATUS：ACTIVE    :在线，可以提供服务
```

# 简单的客户端实现

使用多进程方式实现

- exec函数族函数

  - execl
  - execlp

- 父进程

  - 子进程 -> 执行

    execlp("fdfs_upload_file" , "xx", arg, NULL), 有结果输出, 输出到终端

    - 不让它写到终端 -> 重定向dup2(old, new)
      - old-> 标准输出
      - new -> 管道的写端
      - 文件描述符
      - 数据块读到内存 -> 子进程
        - 数据最终要给到父进程
    - pipe -> 读端, 写端
      - 在子进程创建之前创建就行了

  - 父进程

    - 读管道 -> 内存
    - 内存数据写数据库

![simple_client](E:\projects_code\wokedir\cpp_library\分布式服务器\static\simple_client.png)

```
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <string.h>
#include <fcntl.h>
#include <errno.h>
#include <sys/wait.h>
#include <sys/types.h>
#include <sys/stat.h>
#include "fdfs_client.h"

// 使用提供的API
int my_upload_file1(const char* conf_file,const char* local_file,char* file_id)
{
	char group_name[FDFS_GROUP_NAME_MAX_LEN + 1];
	ConnectionInfo *pTrackerServer;
	int result;
	int store_path_index;
	ConnectionInfo storageServer;

	if ((result=fdfs_client_init(conf_file)) != 0)
	{
		return result;
	}

	pTrackerServer = tracker_get_connection();
	if (pTrackerServer == NULL)
	{
		fdfs_client_destroy();
		return errno != 0 ? errno : ECONNREFUSED;
	}

	*group_name = '\0';
	if ((result=tracker_query_storage_store(pTrackerServer, \
	                &storageServer, group_name, &store_path_index)) != 0)
	{
		fdfs_client_destroy();
		fprintf(stderr, "tracker_query_storage fail, " \
			"error no: %d, error info: %s\n", \
			result, STRERROR(result));
		return result;
	}

	result = storage_upload_by_filename1(pTrackerServer, \
			&storageServer, store_path_index, \
			local_file, NULL, \
			NULL, 0, group_name, file_id);
	if (result == 0)
	{
		printf("%s\n", file_id);
	}
	else
	{
		fprintf(stderr, "upload file fail, " \
			"error no: %d, error info: %s\n", \
			result, STRERROR(result));
	}

	tracker_disconnect_server_ex(pTrackerServer, true);
	fdfs_client_destroy();

	return result;
}


/* 使用多进程的方式实现
创建匿名管道 fd[2] 用于父子进程之间的通信。

如果管道创建失败，会输出错误信息并退出程序。

调用 fork 创建一个新的子进程。在父进程中，fork 返回子进程的 PID，而在子进程中，fork 返回 0。

在子进程中（pid == 0），通过 dup2 将标准输出 STDOUT_FILENO 重定向到管道的写端 fd[1]，这样子进程的标准输出将通过管道写入。

子进程不需要从管道读取数据，所以关闭管道的读端 fd[0]。

使用 execlp 调用 fdfs_upload_file 命令，传递了 conf_file 和 local_file 参数。这里启动了一个新的程序执行上传操作。如果 execlp 失败，会输出错误信息。

在父进程中，关闭管道的写端 fd[1]，因为父进程将从管道读取数据。

使用 read 从管道的读端 fd[0] 读取数据到 file_id 缓冲区中，最多读取 size 个字节的数据，这些数据就是 fdfs_upload_file 命令的标准输出。

最后，父进程通过 wait 回收子进程的 PCB（进程控制块），确保子进程被正确回收。
*/
int my_upload_file2(const char* conf_file,const char* local_file,char* file_id,int size)
{
    // 创建匿名管道用于父子进程通信
    int fd[2];
    int ret=pipe(fd);
    
    if(ret==-1)
    {
        perror("pipe error");
        exit(0);
    }

    // 创建子进程
    pid_t pid=fork();

    // 子进程
    if(pid==0)
    {
        // 标准输出重定向到管道的写端
        dup2(fd[1],STDOUT_FILENO);

        // 子进程从管道写不需要读
        close(fd[0]);

        // 执行execlp命令
        execlp("fdfs_upload_file","xxx",conf_file,local_file,NULL);

        perror("execpl error");
    }
    else
    {
        // 父进程读取管道信息
        close(fd[1]);
        read(fd[0],file_id,size);

        // 回收子进程PCB
        wait(NULL);
    }

}
```

这里介绍下`execlp`函数，函数签名如下：

```
int execlp(const char *file, const char *arg, ...);
```

- `file` 参数是一个字符串，用于指定要执行的程序的文件名。该文件名可以包含可执行文件的绝对路径，或者只包含文件名，这种情况下系统会在 PATH 环境变量指定的目录中查找可执行文件。
- `arg` 和后续的参数是用于传递给新程序的命令行参数。这些参数按顺序排列，并以 NULL 结尾，类似于 C 语言的 `main` 函数的参数。

`execlp` 函数会将当前进程替换为新的程序，新程序从 `file` 参数指定的可执行文件中启动，同时使用 `arg` 参数以及后续参数作为它的命令行参数。它继承了当前进程的 PID（进程标识符）和打开的文件描述符，因此可以在不启动新进程的情况下改变当前进程的行为。

`execlp` 返回的唯一情况是在发生错误时，此时返回 -1，并设置 `errno` 来指示错误的原因。如果函数成功执行，它将不会返回。

# 面试题

**1、什么是FastDFS?**

答：FastDFS是用C语言编写的一款开源的分布式文件系统。FastDFS 为互联网量身定制，充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，使用 FastDFS很容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务。

**2、FastDFS有哪些架构?**

答：FastDFS 架构包括Tracker server 和Storage server。

客户端请求Tracker server进行文件上传、下载，通过 Tracker server 调度最终由 Storage server 完成文件上传和下载。Tracker server 作用是负载均衡和调度，通过 Tracker server 在文件上传时可以根据一些策略找到 Storage server 提供文件上传服务。可以将 tracker 称为追踪服务器或调度服务器。

Storage server 作用是文件存储，客户端上传的文件最终存储在 Storage 服务器上，Storageserver 没有实现自己的文件系统而是利用操作系统 的文件系统来管理文件。可以将storage称为存储服务器。

**3、FastDFS服务端有哪些角色?**

答：Tracker：管理集群，tracker 也可以实现集群。每个tracker节点地位平等，收集Storage集群的状态。

Storage：实际保存文件Storage分为多个组，每个组之间保存的文件是不同的。每个组内部可以有多个成员，组成员内部保存的内容是一样的，组成员的地位是一致的，没有主从的概念。

**4、FastDFS的交互过程是怎样的？**

答：-client询问tracker上传到的storage，不需要附加参数;

-tracker返回一台可用的storage;

-client直接和storage通讯完成文件上传。

**5、FastDFS有哪些优点？**

答：Fastdfs优点：
（1）主备Tracker服务，增强系统的可用性；
（2）系统不需要支持POSIX，这样的话就降低了系统的复杂度，使得处理的速度会更高；
（3）支持主从文件，支持自定义扩展名；
（4）支持在线扩容机制，增强了系统的可扩展性；
（5）实现了软RAID，增强了系统的并发处理能力和数据容错恢复能力

**6、FastDFS有哪些缺点？**

答：FastDFS缺点：
（1）通过API下载，存在单点的性能瓶颈；
（2）不支持断点续传，对大文件将是噩梦；
（3）同步机制不支持文件正确性校验，降低了系统的可用性；
（4）不支持POSIX通用接口访问，通用性比较的低；
（5）对跨公网的文件同步，存在着比较大的延迟，需要应用做相应的容错策略

**7、FastDFS特性有哪些?**

答：（1）Tracker服务器是整个系统的核心枢纽，其完成了访问调度(负载均衡)，监控管理Storage服务器，由此可见Tracker的作用至关重要，也就增加了系统的单点故障，为此FastDFS支持多个备用的Tracker，虽然实际测试发现备用Tracker运行不是非常完美，但还是能保证系统可用。

（2）在文件同步上，只有同组的Storage才做同步，由文件所在的源Storage服务器push至其它Storage服务器，目前同步是采用Binlog方式实现，由于目前底层对同步后的文件不做正确性校验，因此这种同步方式仅适用单个集群点的局部内部网络，如果在公网上使用，肯定会出现损坏文件的情况，需要自行添加文件校验机制。

（3）支持主从文件，非常适合存在关联关系的图片，在存储方式上，FastDFS在主从文件ID上做取巧，完成了关联关系的存储。

**8、Nginx环境调用FastDFS的要求是什么？**

答：（1）编译安装nginx时附带fastdfs-nginx-module模块 –add-module=/root/fastdfs-nginx-module/src；（2）修改nginx配置文件增加 ngx_fastdfs_module;（3）编译fastdfs_client.so php扩展模块；（4）修改php.ini 让php-fpm 支持fastdfs_client 扩展模块 extension = fastdfs_client.so。

**9、FastDFS适用的场景以及不适用的场景？**

答：FastDFS是为互联网应用量身定做的一套分布式文件存储系统，非常适合用来存储用户图片、视频、文档等文件。对于互联网应用，和其他分布式文件系统相比，优势非常明显。FastDFS没有对文件做分块存储，因此不太适合分布式计算场景。

**10、 FastDFS 如何现在组内的多个 storage server 的数据同步？**

当客户端完成文件写至 group 内一个 storage server 之后即认为文件上传成功，storage server 上传完文件之后，会由后台线程将文件同步至同 group 内其他的 storage server。后台线程同步参考的依据是每个 storageserver 在 写完文件后，同时会写一份 binlog，binlog 中只包含文件名等元信息,storage 会记录向 group 内其他 storage 同步的进度，以便重启后能接上次的进度继续同步；进度以时间戳的方式进行记录，所以最好能保证集群内所有 server 的时钟保持同步。

**11、FastDFS的流程**

- **选择 tracker server**

当集群中不止一个 tracker server 时，由于 tracker 之间是完全对等
的关系，客户端在 upload 文件时可以任意选择一个 trakcer。

- **选择存储的 group**

当 tracker 接收到 upload file 的请求时，会为该文件分配一个可以
存储该文件的 group，支持如下选择 group 的规则：

1. Round robin，所有的 group 间轮询。
2. Specified group，指定某一个确定的 group。
3. Load balance，剩余存储空间多多 group 优先。
4. 选择 storage server。

- **选择 storage server**

233当选定 group 后，tracker 会在 group 内选择一个 storage server
给客户端，支持如下选择 storage 的规则：

1. Round robin，在 group 内的所有 storage 间轮询。
2. First server ordered by ip，按 ip 排序。
3. First server ordered by priority，按优先级排序（优先级在
   storage 上配置）。

- 选择 storage path

当分配好 storage server 后，客户端将向 storage 发送写文件请求，
storage 将会为文件分配一个数据存储目录，支持如下规则：

1. Round robin，多个存储目录间轮询。
2. 剩余存储空间最多的优先。

- 生成 Fileid

选定存储目录之后，storage 会为文件生一个 Fileid，由 storage server ip、文件创建时间、文件大小、文件 crc32 和一个随机数拼接而成，然后将这个二进制串进行 base64 编码，转换为可打印的字符串。

- 选择两级目录

当选定存储目录之后，storage 会为文件分配一个 fileid，每个存储目录下有两级 256*256 的子目录，storage 会按文件 fileid 进行两次 hash （猜测），路由到其中一个子目录，然后将文件以 fileid 为文件名存储到该子目录下。

- 生成文件名

  当文件存储到某个子目录后，即认为该文件存储成功，接下来会为234该文件生成一个文件名，文件名由 group、存储目录、两级子目录、fileid、 文件后缀名（由客户端指定，主要用于区分文件类型）拼接而成。

# 引用

https://zh.wikipedia.org/zh-hans/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F

https://lfool.gitbook.io/operating-system/di-si-zhang-wen-jian-guan-li/3.-wen-jian-de-wu-li-jie-gou

https://www.bilibili.com/read/cv7455762/

https://zhuanlan.zhihu.com/p/577873906